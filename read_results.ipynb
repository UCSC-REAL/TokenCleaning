{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from lm_eval.utils import make_table\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "eval_root_path=\"eval_results\"\n",
    "TASK_LISTS=['mmlu', 'bbh', 'gsm8k', 'truthfulqa_mc2', \"arc_challenge\", \"piqa\", \"hellaswag\", \"openbookqa\", \"triviaqa\", 'sciq', 'arc_easy', 'logiqa', 'boolq', 'winogrande'] ##task\n",
    "\n",
    "\n",
    "base_model=\"meta-llama/Llama-3.2-3B\" #\"meta-llama/Llama-3.1-8B\" \"mistralai/Mistral-7B-v0.3\"\n",
    "data_prop = 0.6\n",
    "\n",
    "##eval model name\n",
    "model_tags=[\"ds2-50k-self-evolving\"]\n",
    "\n",
    "\n",
    "results_all = {}\n",
    "for model_tag in model_tags:\n",
    "    eval_result_path = f\"{eval_root_path}/{os.path.basename(base_model)}/{data_prop}/{model_tag}/\"\n",
    "\n",
    "    if model_tag != 'base':\n",
    "        exp_files = os.listdir(eval_result_path)\n",
    "        for file_name in exp_files:\n",
    "            if str(data_prop) in file_name and os.path.basename(base_model) in file_name: \n",
    "                log_path = file_name\n",
    "    else:\n",
    "        log_path = os.listdir(eval_result_path)[0]\n",
    " \n",
    "    json_files = os.listdir(os.path.join(eval_result_path, log_path))\n",
    "\n",
    "    results = {}\n",
    "    for file in json_files:\n",
    "        with open(os.path.join(eval_result_path, log_path, file), 'r') as f:\n",
    "            temp = json.load(f)\n",
    "            for task in TASK_LISTS:\n",
    "                if task in temp['results'].keys():                    \n",
    "                    if task in ['hellaswag', 'piqa', 'openbookqa', 'arc_challenge', 'mmlu', 'truthfulqa_mc2', 'sciq', 'arc_easy', 'logiqa', 'boolq', 'winogrande']:\n",
    "                        metric = 'acc,none'\n",
    "                    elif task == 'gsm8k':\n",
    "                        metric = 'exact_match,strict-match'\n",
    "                    elif task == \"triviaqa\":\n",
    "                        metric = \"exact_match,remove_whitespace\"\n",
    "                    elif task == 'bbh':\n",
    "                        metric = 'exact_match,get-answer' \n",
    "                    results[task] = temp['results'][task][metric]\n",
    "\n",
    "    ## load tydiqa result \n",
    "    tydiqa_result_file = os.path.join(eval_result_path, \"metrics.json\")\n",
    "    if os.path.exists(tydiqa_result_file):\n",
    "        with open(tydiqa_result_file, 'r') as f:\n",
    "            results['tydiqa'] = round(json.load(f)['average']['f1'] / 100, 4)\n",
    "\n",
    "    results_all[model_tag] = results\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame.from_dict(results_all, orient='index')\n",
    "TASK_LISTS=[\"truthfulqa_mc2\", \"tydiqa\", 'logiqa', 'mmlu',  \"hellaswag\", \"arc_challenge\", \"boolq\"]\n",
    "\n",
    "results_df = results_df[TASK_LISTS]\n",
    "results_df = results_df.map(lambda x: round(100*x, 2) if pd.notnull(x) else x)\n",
    "results_df['Average'] = results_df.mean(axis=1).round(1)\n",
    "\n",
    "print(\"\\nResults DataFrame (Reordered with Average, Percentage Format):\\n\")\n",
    "results_df = results_df.reindex(model_tags)\n",
    "print(results_df.to_string(line_width=1000))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
